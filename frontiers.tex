%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is just an example/guide for you to refer to when submitting manuscripts to Frontiers, it is not mandatory to use Frontiers .cls files nor frontiers.tex  %
% This will only generate the Manuscript, the final article will be typeset by Frontiers after acceptance.   
%                                              %
%                                                                                                                                                         %
% When submitting your files, remember to upload this *tex file, the pdf generated with it, the *bib file (if bibliography is not within the *tex) and all the figures.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Version 3.3 Generated 2016/11/10 %%%
%%% You will need to have the following packages installed: datetime, fmtcount, etoolbox, fcprefix, which are normally inlcuded in WinEdt. %%%
%%% In http://www.ctan.org/ you can find the packages and how to install them, if necessary. %%%
%%%  NB logo1.jpg is required in the path in order to correctly compile front page header %%%

\documentclass[utf8]{frontiersSCNS} % for Science, Engineering and Humanities and Social Sciences articles
%\documentclass[utf8]{frontiersHLTH} % for Health articles
%\documentclass[utf8]{frontiersFPHY} % for Physics and Applied Mathematics and Statistics articles

%\setcitestyle{square} % for Physics and Applied Mathematics and Statistics articles
\usepackage{url,hyperref,lineno,microtype,subcaption}
\usepackage[onehalfspacing]{setspace}



\newcommand{\RN}[1]{%
  \textup{\uppercase\expandafter{\romannumeral#1}}%
}


% Leave a blank line between paragraphs instead of using \\


\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{} %use et al only if is more than 1 author
\def\Authors{Mehdi Bayati\,$^{1,2}$, Torsten Neher\,$^{3}$, Laurenz Wiskott\,$^{1}$ and Sen Cheng\,$^{1,2,*}$}
% Affiliations should be keyed to the author's name with superscript numbers and be listed as follows: Laboratory, Institute, Department, Organization, City, State abbreviation (USA, Canada, Australia), and Country (without detailed address information such as city zip codes or street names).
% If one of the authors has a change of address, list the new address below the correspondence details using a superscript symbol and use the same symbol to indicate the author in the author list.
\def\Address{$^{1}$Institut f\"ur Neuroinformatik, Ruhr-Universit\"at Bochum, D-44801 Bochum, Germany \\
$^{2}$Mercator Research Group 'Structure of Memory', Ruhr-University Bochum, Bochum, Germany \\
$^{3}$Mental Health Research and Treatment Center, Department of Clinical Child and Adolescent Psychology, Faculty of Psychology, Ruhr University Bochum, Bochum, Germany}
% The Corresponding Author should be marked with an asterisk
% Provide the exact contact address (this time including street name and city zip code) and email of the corresponding author
\def\corrAuthor{Corresponding Author}

\def\corrEmail{sen.cheng@rub.de}


\begin{document}
\onecolumn
\firstpage{1}

\title{Storage Fidelity for Sequence Memory in the Hippocampal Circuit} 

\author[\firstAuthorLast ]{\Authors} %This field will be automatically populated
\address{} %This field will be automatically populated
\correspondance{} %This field will be automatically populated

\extraAuth{}% If there are more than 1 corresponding author, comment this line and uncomment the next one.
%\extraAuth{corresponding Author2 \\ Laboratory X2, Institute X2, Department X2, Organization X2, Street X2, City X2 , State XX2 (only USA, Canada and Australia), Zip Code2, X2 Country X2, email2@uni2.edu}


\maketitle
Running title: Sequence Memory Storage in the Hippocampal Circuit\\
Number of text pages: 28\\
Number of figures: 13\\
Corresponding author: Sen Cheng, Institut f\"ur Neuroinformatik, Ruhr-Universit\"at Bochum, D-44801 Bochum, Germany\\
Grant sponsor: DFG; Grant number: SFB874-Project B2 (S.C.)\\
Grant sponsor: DFG; Grant number: SFB874-Project B3 (L.W.)\\
Grant sponsor: Stiftung Mercator (S.C.)\\
Keywords: Hippocampus, Neural sequences, Episodic memory, Neural Networks, Feedforward Networks\\

\newpage

\linenumbers

\begin{abstract}

%%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details.
\section{}
Despite extensive research, the role of the hippocampus in episodic memory storage and recall is still unclear. 
%
We have recently proposed that episodic memories are best represented by temporal sequences of neural activation patterns and that the hippocampal circuit is optimized to store these sequences.
%
Here, we study the possible mechanisms by which memory sequences can be stored and recalled from the cortico-hippocampal circuit, consisting of the EC-CA3-CA1-EC loop.
%
Storing sequences presents entirely different challenges from storing static patterns.    
%
During memory encoding, CA3 sequences are hetero-associated with EC sequences, which are driven by sensory inputs.
%
CA3 sequences are generated either intrinsically or via blending the EC inputs and CA3 recurrent inputs.  
%
During memory retrieval, CA3 sequences have to be reactivated based on partial, noisy cues, which are provided to EC. 
%
The retrieved sequences in CA3 then reactivate the stored patterns in EC via the CA1 layer. 
%
We find that memory performance depends on the networkâ€™s ability to perform pattern completion of individual patterns and robust retrieval of sequences from CA3.
%
These two functions have competing requirements.
%
Modeling CA3 as a fixed randomly connected network facilitates decoding, but sequence retrieval in CA3 fails if any noise is present.
%
On the other hand, using a fixed locally connected network, the stored sequences are retrieved robustly, but the correlations between successive patterns impair pattern completion when decoding the CA3 patterns. 
%
Combining the advantages of both models, networks, that generate mutually uncorrelated pattern sequences and allow a hetero-association between them, achieve a good overall memory performance, because sequences in CA3 are encoded robustly and do not impair decoding in the feedforward connections to CA1 and EC.
%
The dynamics of the later network can be driven jointly by the CA3 connections and the EC inputs.
%
Our results suggest that the contribution of the recurrent CA3 connections on the CA3 dynamics is essential during learning, and that it removes correlations between the EC input patterns and allow an efficient sequence memory storage.
%
In conclusion, the cortico-hippocampal circuit can robustly store and retrieve sequences of patterns, but  memory performance critically depends on the network architecture in CA3. 
%
% \keyFont{ \section{Keywords:} keyword, keyword, keyword, keyword, keyword, keyword, keyword, keyword} %All article types: you may provide up to 8 keywords; at least 5 are mandatory.
\end{abstract}


\section*{Introduction}
Memory is not a unitary system, but is rather a collection of several different systems that, in some cases involves distinct regions of the brain. Tulving (1972) has suggested episodic memory as a separate memory system that stores memories of personally experienced events \cite{tulving1972episodic}. The hippocampus has been implicated in the acquisition and consolidation of new episodic memories in humans. Patients with damage to the hippocampus and nearby brain areas suffer from severe anterograde amnesia \cite{scoville1957loss, milner1968further}. Hippocampal lesions in animal models impair learning and memory, too. For instance, hippocampal rats are impaired at associating time-delayed stimuli \cite{gluck2001gateway} and the object-cued retrieval of paired associate memory, even in the absence of a delay \cite{yoon2012hippocampus}. Most prominently, rats have severe deficits in spatial learning after hippocampal lesions \cite{morris1982place}.

Multiple studies implicate the hippocampus in temporal sequence learning. Rats with hippocampal lesions have difficulty remembering sequences of spatial locations \cite{chiba1994memory} and hippocampal lesions impair a rat's ability to learn which odor came first in sequence of odors \cite{fortin2002critical}. Agster et al. showed that hippocampal rats had deficits disambiguating the overlapping odor sequences \cite{agster2002hippocampus}. After rats run through the place fields of hippocampal CA1 place cells causing the place cells to fire in a fixed order, the cells become active in the same sequences during sleep \cite{lee2002memory}. A number of other studies have also found such replay of temporal sequences in the hippocampus during sleep \cite{louie2001temporally, kudrimoti1999reactivation, qin1997memory, skaggs1996replay}.      

However, it remains controversial, whether nonhuman animals possess the same capacity for episodic memory as humans do \cite{suddendorf2007evolution, cheng2016dissociating}. Some authors have adopted the view that the essential aspect of episodic memory is the information about the what, where, and when of an experienced event, and have been able to show that a variety of species are able to store such  memories\cite{babb2006episodic, clayton1998episodic, dally2006food, zentall2001episodic, dere2005episodic, kaminski2008prospective, hoffman2009memory}. Nevertheless, even these authors refer to the memory of what-where-when as episodic-like memory in recognition that human episodic memory might be more, or different, than that.

Another important issue is how the hippocampal circuit, whose structure is preserved across all mammals \cite{allen2013evolution}, stores and retrieves memories. A number of theories have been proposed \cite{treves1992computational, marr1991simple, mcclelland1995there, HIPO:HIPO450020209}. 
Based on its anatomical and physiological properties the hippocampus can be divided into the DG, which includes a large number of small granule cells with low activity \cite{leutgeb2007pattern}, and the CA3, CA2 and CA1 regions consisting of a homogeneous set of pyramidal cells. CA3 is famous for its recurrent collaterals \cite{ishizuka1990organization, li1994hippocampal}, and the connections between the subregions are established in a feedforward manner \cite{amaral1990chapter}. Over the last decades, a standard framework has emerged and has been tested regarding hippocampal functioning (for example see \cite{fontanari1995model}). It postulates that cortical inputs drive plasticity in the recurrent CA3 synapses to store patterns rapidly in its recurrent connections. Hence, the CA3 region functions as an auto-associative memory \cite{marr1991simple, mcnaughton1987hippocampal, treves1994computational, o1994hippocampal}. The feedforward structure of the connectivity between the hippocampal subregions \cite{amaral1990chapter} has been less in focus until recently \cite{neher2015memory, pyka2014pattern}.  

%this and the next paragraph are moved here from discussion
The standard framework postulates that CA3 with its massive recurrent connections behaves as an attractor network, performing pattern completion when a partial and noisy cue is provided \cite{mcnaughton1987hippocampal, rolls2007attractor}. Experimental studies support the auto-associative memory function of CA3. For instance, rats with lesioned CA3 are impaired in remembering a location when parts of the spatial cues are removed \cite{gold2005role}. Another study argues that spatial pattern completion requires plasticity in the recurrent CA3 synapses \cite{nakazawa2002requirement}. However, the standard framework has several weaknesses and cannot account for some recent observations in the hippocampal formation. 


Secondly, the standard framework cannot explain offline sequential activity (OSA) in awake animals \cite{cheng2013crisp}, which was found to be important for spatial learning \cite{jadhav2012awake}. The standard framework assumes that cortical inputs drive plasticity in the recurrent CA3 synapses to rapidly imprint memories as attractor states in CA3. However, this assumption is not consistent with recent experimental findings on OSA in the awake state. Novel sequences that were never experienced by the animal are played out independently of the preceding experience \cite{gupta2010hippocampal} (for a review, see \cite{dragoi2011preplay}). These sequences are potentially used to predict immediate future behaviour and are generated even in cases, in which the specific combination of start and goal locations is novel \cite{pfeiffer2013hippocampal}. All together, these sequential activities, perhaps representing episodic memories, appear to be intrinsic to the hippocampal network rather than imprinted by external inputs \cite{cheng2013crisp}. Recently proposed computational models tried to explain how CA3 might generate these intrinsic sequences to account for preplay \cite{azizi2013computational, romani2015short}. 



Addressing both aforementioned issues, we have recently proposed that episodic memory is best thought of as sequences of activity patterns \cite{cheng2016episodic} and that the hippocampal circuitry, including the feedforward projections  between regions and the recurrent network in CA3, has been optimized for the storage of pattern sequences \cite{cheng2013crisp}. One of the main features of this theory, called CRISP, is that plasticity in the recurrent CA3 is not necessary for rapid learning and synaptic weights remain relatively fixed during memory storage \cite{nakazawa2003hippocampal, cravens2006ca3}. In CRISP, neural sequences are intrinsic to CA3. To store episodic memories, sequences of external input patterns are mapped onto these intrinsic sequences through synaptic plasticity in the feedforward projections \cite{willshaw1969non} (see section Materials and Methods for details).
Many models for generating activity sequences in recurrent networks have been established and tested computationally \cite{sussillo2009generating, lazar2009sorn, rajan2016recurrent, jaeger2001echo, kropff2007complexity, bayati2015self}. However, the robustness of these models in regenerating the sequences with respect to cue noise has rarely been studied.


In order to help understanding this study, we suggest the reader to consult other previously published papers on the topic \cite{cheng2013crisp, neher2015memory}
%
This study relies on the contents of the CRISP theory of hippocampal function in episodic memory \cite{cheng2013crisp}. 
% 
Here we implement the important features of the CRISP theory in an abstract neural network model to store and retrieve episodic memories. We use a cortico-hippocampal circuit consisting of the EC-CA3-CA1-EC loop. 
%
First, different recurrent network architectures are tested for their ability to robustly generate sequences of activity patterns. 
%
Then these recurrent networks are embedded in the hippocampal circuit to study storage and retrieval in the complete circuit.
%
Our neural network architecture is largely based on our previous work \cite{neher2015memory}, which in turn was derived from Rolls (1995) \cite{fontanari1995model}, with the important exception of the CA3 recurrent dynamics. 
%
While our previous work focused on storing static patterns, here we use a similar network architecture to store sequences in the hippocampal network. Sequence memory storage comes with entirely different challenges, requires different analyses and reveals different insights into the role of neural dynamics in CA3. 

% authors may use "Analysis" 
\section*{Materials and Methods}




\subsection*{Model Architecture and Activation Function}


The model includes the entorhinal cortex (EC), CA3 and CA1. 
%
%Kloosterman et al. showed that electrical stimulation of sites within the CA1 field evokes activity concurrently in both the superficial and deep layers of the EC. This findings suggest that the deep and superficial layers of the EC act as a single functional entity, rather than as separate structures \cite{kloosterman2000functional}.
%
Recent findings indicate that reciprocal interactions between deep and superficial layers of the EC are quite substantial \cite{canto2008does, van2003morphological}. Furthermore, main cortical inputs target both deep and superficial layers. Thus, the deep and superficial layers of the EC might act as a single functional entity, rather than as separate structures \cite{kloosterman2000functional}.
%
On this account, the superficial and deep layers of the EC are clamped to the same layer. Please notice that this does not mean that we close the loop, namely the activities in the EC outputs do not propagate, via the EC input, through the network.

The number of neurons $N$ in each region and connections a neuron in a downstream region forms with neurons in the upstream region are summarized in Fig.~\ref{Fig_1}. The parameter $N$ is chosen based on anatomical data from the rat hippocampal formation \cite{amaral1990chapter, cutsuridis2010hippocampal} and is scaled down by the factor of $100$ in order to reduce the computational cost (see \cite{neher2015memory} for details). The connectivity parameter is set to $\% 32$. 
We run the simulation for more diluted and dense connectivity, but the results do not change. 

\begin{figure}[!htb]
\centering\includegraphics[width=1.\linewidth]{Fig_1.eps}
\caption{\textbf{Model Overview.} \textbf A: The three subregions EC, CA3 and CA1 are modeled. $a$ denotes the proportion of cells being active at any given time. Arrows indicate connectivity among regions. Solid black lines indicate fixed random connections, solid green line represent plastic connections that are adjusted during learning, and dashed lines show connections that could be either fixed (using hand-wired models for CA3) or plastic (using EC-input and intrinsic input to train CA3 weights). The number next to the arrows show the number of connections a cell in the downstream region has with cells in the upstream region.}
\label{Fig_1}
\end{figure}


Neurons in our model are binary, i.e., they either fire or are silent reflected by a value of $1$ or $0$, respectively \cite{fontanari1995model}.

First, the activation $h^i$ of the receiving cell $i$ is calculated as the weighted sum of its inputs $u^j$
\begin{align}
	\label{activation}
	h^i = \sum_{j=1}^{N_\mathrm{in}} w^{ij}u^j,
\end{align}  
where $w^{ij}$ is the strength of the connection from cell $j$ to cell $i$ and is defined as $0$ whenever this connection is not existent. Second, a $k$-Winner-Take-All (WTA) mechanism is applied to determine which cells become active. The $k$ cells with the highest activation are set to $1$ and the others are inhibited and thus set to $0$, i.e.,
\begin{eqnarray}
\label{eq:kWTA}
	\kappa &:& \mathbb{R}_+^N \times \mathbb{N} \to \{0,1\}^N \\
	\kappa^i (h;k) &=& \left \{ \begin{array}{ll}
			1 &\text{ if $h^i$ is among the $k$ highest } \{ h^j:1\le j\le N \}. \\
			0 &\text{ otherwise}.\\
	\end{array} \right.
	\label{eq:binary}
\end{eqnarray}
The number k is chosen uniformly from the interval $ [aN - \delta , Na + \delta ]$. The parameter $a$ is the sparsity of the activity in the corresponding region and $\delta = \% 10 N$. This ensures that a different number of k cells is recruited for every pattern (Fig.~\ref{Fig_1}). 

Thus, inhibitory cells are not modeled explicitly but rather through their effect on a population level \cite{renno2010mechanism, roudi2008representing, moustafa2009neurocomputational, appleby2011role, monaco2011modular}.


\subsection{Models of CA3}
\label{ca3:models}


We explore three models for CA3, in which the synaptic weights are either hand-wired and fixed (first two models) or plastic (third model). 

Another important aspect in CRISP is whether the CA3 dynamics generate sequences intrinsically during storage (first two models) or CA3 activity patterns can be driven partly by EC inputs, as well (third model). 

These models are described below.
In all networks, non-existing connections are modelled as connections with zero weight.

\begin{enumerate}
\item \textit{Randomly connected network (RCN):} Each CA3 node is connected randomly to $32\%$ of the other nodes. The weights for the connections are sampled from a uniform distribution between zero and one. The activation $h^i$ of the receiving cell $i$ is determined by

\begin{align}
	\label{activationRCN}
	h^i = \sum_{j=1}^{N^{CA3}_\mathrm{in}} w^{ij}y^{j}_{t-1}.
\end{align}  


\item \textit{Locally connected network (LCN):} Each CA3 node is assigned a virtual location in a 2-d square environment and connected to $800$ of its nearest neighbours. The weights of these connections are assigned according to a Gaussian kernel based on the distance between cells \cite{azizi2013computational}. 
Such a network generates bump shape activity. We use the adaptation parameter ($0 \leq J \leq 1$) to make the bump of activity (the attractor of the network) unstable and move it through the network. This parameter can control the speed of the bump. 
Therefore, the activation $h^i$ of the receiving cell $i$ depends on its activity in the previous time step and the weighted sum of its inputs $y^j$.
\begin{align}
\label{activationLCN}
h^{i}_{t} = (1 - J y^{i}_{t-1}) \sum_{j = 1}^{N^\mathrm{CA3}_\mathrm{in}} {{w^{ij} y^{j}_{t-1}}}
\end{align}

\item \textit{Dual-driven network (DDN):} The connectivity is established like for a RCN.

CA3 activity patterns are driven jointly by EC inputs and CA3 collateral inputs \textit{during the learning phase}.

We use the parameter ($0 \leq \alpha \leq 1$) to control the contribution of these inputs in the activity of each cell. 

Therefore, the activation $h^i$ of the receiving cell $i$ depends on the activity of the CA3 network in the previous time step and the concurrent activity in the EC.
\begin{align}
\label{activationDDN}
h^{i}_{t} = (1 - \alpha) \sum_{j = 1}^{N^{CA3}_\mathrm{in}} {{w^{ij} y^{j}_{t-1}}} + \alpha \sum_{j = 1}^{N^{EC}_\mathrm{in}} {{w^{ij} u^{j}_{t}}} 
\end{align}
Therefore, when $\alpha = 0$ the network activity is driven intrinsically. On the other hand, when $\alpha = 1$, the network activity is driven merely by EC inputs. The in-between $\alpha$ values allow contribution of both inputs based on Eq.~\ref{activationLCN}.        

The CA3 recurrent network learns the sequences through successive hetero-associations (see subsection~\ref{learning}).
\end{enumerate}

During the learning phase, input patterns from EC are hetero-associated with network states in CA3. In all models, CA3 dynamics has to be triggered. Once initialized, the next pattern in CA3 is generated according to Eqs.~\ref{activationRCN}, \ref{activationLCN}, or \ref{activationDDN}, and \ref{eq:kWTA}.

The initialization pattern is adjusted according to the CA3 model: for the RCN and DDN, it is a random pattern; for the LCN, it is a local bump-shape pattern in a random location.

%The feedforward connections between different areas are set randomly and the initial weights are sampled from a uniform distribution between zero and one.

\subsection{Input Statistics}

To test the ability of each network to store memory sequences, we generate $P = L \times M$ patterns, where $L$ is the number of sequences, each with $M$ patterns. We denote the set of input patterns as 
\begin{equation}
	\{ u_{l,m}: 1\le l \le L, 1\le m \le M \}
\end{equation}
Since we recently found that the statistics of the stored patterns have a large impact on the memory performance of a network \cite{neher2015memory}, we consider the more realistic entorhinal grid cell input patterns to EC \cite{hafting2005microstructure}. 



%\textit{Random Patterns:} In each pattern, the activity of $k$ randomly chosen cells is set to one, all others are set to zero. 


%\textit{Grid Cell patterns:}
A number of cells in the medial entorhinal cortex (MEC) of many species, called grid cells, are place-modulated neurons with discrete firing fields arranged in a periodic hexagonal lattice \cite{hafting2005microstructure}.  

The grid cell properties are extracted from~\cite{stensola2012entorhinal} to match the experimental data.
Each cell is equipped with a hexagonal grid of place fields with equal size.

Motivated by the findings of Stensola et al. (2012), we divided the grid cell population into four modules \cite{stensola2012entorhinal}. Cells belonging to the same module have similar grid spacing and orientation, but different spatial phases (!!!), which were drawn from normal distributions. 

The mean grid spacings $s_i$ and orientations of the modules are 38.8, 48.4, 65, 98.4 cm, and 15, 30, 45, 60 degrees. These parameters are drawn from normal distributions with variances 8 cm and 3 degree, respectively.

See \cite[Fig 1B-1C]{neher2015memory} for the resulting distribution of spacings and orientations of the population.

Most grid cells ($87\%$) belong to the two modules with small spacings \cite{stensola2012entorhinal}. 

The activation of grid cell $i$ at location $\mathbf{r}=(x,y)$ is determined by
\begin{equation}
\label{eq:grid}
h^i(\mathbf{r}) = A^{ij} \exp \left[ -\ln(5) \left(\frac{d(\mathbf{r})}{\sigma^i}\right)^2 \right],
\end{equation}
where $d$ is the Euclidean distance to the nearest field center $j$ and $\sigma^i$ is the radius of the firing field.

Each field has the same size, which is related to the grid spacing via $ \sigma_i  = 0.32 s_i $ (see Fig
S4G in \cite{hafting2005microstructure}). 

$A^{ij}$ is the peak firing rate of the cell in the center of a field and reaches $0.2 A^{ij}$ at the border, which is motivated by the definition of a place field \cite{hafting2005microstructure}. The peak firing rates are drawn from a uniform distribution with from 0.5 to 1.5 (see \cite{neher2015memory} for a visualization of grid patterns).  

To generate inputs, we built a $1\textrm{m} \times 1\textrm{m}$ virtual square environment, which is discretized into a 30 x 30 grid resulting in 900 x, y coordinates.   
To generate a pattern sequence, we select $M$ locations from a continuous trajectory of adjacent locations in the environment, which mimics the movement of the virtual rat. To generate a binary activity pattern, at each location the $k$ cells with the highest activation are set to one, all others are set to zero according to Eqs.~\ref{eq:kWTA}-\ref{eq:binary}.


\subsection{Learning Phase}
\label{learning}
Our goal is to store the sequences $\mathbf u$ in the model such that they can be retrieved as accurately as possible. To store a sequence of patterns in the network during the learning phase, the plastic weights between subregions (green arrows in Fig.~\ref{Fig_1}) are adjusted according to the Hebbian learning rule (Eq.~\ref{heteroEq}). 
%
The weights in CA3 are plastic only for DDN model.

To store sequences, we first apply the input patterns $u_{l,m}$ to EC. The activities in CA3 are generated intrinsically in the case of RCN, LCN as described in section Models of CA3. The sequence of CA3 generated patterns $\mathbf y$ are then hetero-associated with the EC inputs $\mathbf u$ (Eq.~\ref{heteroEq}). 
%
In the DDN network, firstly the CA3 patterns are driven jointly by CA3 intrinsic inputs and EC inputs, based on Eq.~\ref{activationDDN}). Then, the successive patterns in the CA3 and the corresponding patterns between the CA3 and EC are hetero-associated based on the Eqs.~\ref{heteroca3Eq} and \ref{heteroca3Eq}, respectively.
%
Neural activities in CA1, $\mathbf x$, are triggered by the constant EC input weights via Eqs.~\ref{activation}-\ref{eq:binary} (Fig.~\ref{Fig_2}A). Furthermore, the patterns in CA3 are hetero-associated with the patterns in CA1, and the CA1 patterns with input patterns $\mathbf u$ in the EC output (Eq.~\ref{heteroEq}). 

For hetero-association of a pre-synaptic pattern $a$ with post-synaptic pattern $b$, we use the so-called Stent-Stinger rule \cite{stent1973physiological}
%
\begin{align}
	\label{heteroEq}
	w^{ij} = c^{ij}\sum_{l=1}^L{\sum_{m=1}^M(a^j_{l, m}  - \bar {a}^j)b_{l, m}^i}.
\end{align}
$C$ denotes the connection matrix between two regions, i.e., $c^{ij} = 1$ if there is a connection from cell $j$ to $i$ and $c^{ij} = 0$ otherwise. It insures that non-existing connections remain at zero weight. $\bar{a}^j$ is the mean activity level of the pre-synaptic cell over all sequences. 

During the learning phase for the DDN, we adjust the recurrent weights $V$ in CA3 according to the co-variance rule \cite{sejnowski1977storing} to learn an hetero-association among a set of patterns in a sequence $\{ y_{l, m}: 1\le l \le L, 1\le m \le M\}$.
\begin{align}
	\label{heteroca3Eq}
	v^{ij} =  c^{ij}\sum_{l=1}^L{\sum_{m=1}^{M-1}(y^j_{l, m}  - \bar {y}^j)(y_{l, m+1}^i - \bar{y}^i)}.
\end{align}
By subtracting the mean the two learning rules model LTP and LTD. Furthermore the subtraction is essential for computational reasons (see for example \cite[chapter 8.2]{amit1992modeling}).

After applying the learning rules, the Euclidean norm of the vector $w^i$ of incoming weights to cell $i$, in all layers, is normalized to one to assure that not always the same cells are activated. 

\begin{figure}[!htb]
\centering\includegraphics[width=1.\linewidth]{Fig_2.eps}
\caption{\textbf{Memory storage and retrieval} \textbf A: To store a sequence $(u_{l,1} , . . . ,u_{l,M})$ that represents an episodic memory, a sequence $(y_{l,1} , . . . ,y_{l,M})$ is activated in CA3 and each element $u_{l,t}$ is associated with a particular CA3 state $y_{l,t}$. Furthermore, when the DDN model is used, the successive states of the CA3 are associated together. The dashed lines on the left hand side of \textbf{A} indicate the associations between patterns.  
\textbf{B:} Retrieval of a stored memory sequence from CA3 based on a partial input cue $u^{'EC}_{l,t}$. The retrieved elements (patterns) are noisy and are cleaned up by the CA1â€“EC network.}
\label{Fig_2}
\end{figure}

\subsection{Retrieval Phase}
\label{S:4}

After all sequences are stored in the network, we initiate recall by setting EC to a noisy recall cue $u'_{l, 1}$.  The recall cue is generated by modifying the first pattern of the stored sequence $u_{l, 1}$, such that a number of active neurons is randomly set to be in-active and the same number of in-active neurons is set to be active. 

Therefore, the cue pattern and its corresponding original one have always the same number of active neurons. 

The quality of the recall cue is controlled by the number of cells that fire incorrectly and is measured by the Pearson correlation between the original pattern and the recall cue (see below). We test the model performance with $6$ different recall cue qualities which range between 0 and 1, namely $ u'_{l, 1} \in \{ 0, .2, .4, .6, .8, 1 \} $.       
%
The recall cue triggers a pattern $\tilde{y}_{l, 1}$ in CA3 directly via the previously learned weights from EC to CA3 and subsequently generates an intrinsic sequence ($\tilde{y}_{l, 2}, \tilde{y}_{l, 3}, \ldots$). This sequence is transferred to CA1 ($\tilde{x}_{l, 1}, \tilde{x}_{l, 2}, \ldots$) and from CA1 back to EC ($\tilde{u}_{l, 1}, \tilde{u}_{l, 2}, \ldots$). Fig.~\ref{Fig_2}B illustrates the retrieval process in our model. 


\subsection{Analysis}

\subsubsection{Retrieval Quality}
To measure how well a retrieved pattern matches the stored pattern, we use the Pearson correlation between the originally stored pattern $a_{l, m}$ of a sequence  and the retrieved one $\tilde{a}_{l, m}$. It is defined as
\begin{align*}
	Corr(a_{l, m},\tilde{a}_{l, m})  = \frac{(a_{l, m} -\bar{a})^T(\tilde{a}_{l, m} -\bar{\tilde{a}})}
{\lVert{a_{l, m} -\bar{a}} \rVert \cdot \lVert{\tilde{a}_{l, m} -\bar{\tilde{a}}}\rVert },
\end{align*}     
where $\bar{a}$ and $\bar{\tilde{a}}$ are the means of the stored and retrieved patterns over all sequences, $l = 1,2, ..., L$ and $m = 1,2, ..., M$, respectively. The higher the value of this correlation is, the more similar the recalled pattern is to the original one. We refer to the retrieval quality in CA3, CA1 and the output in EC as $Corr_{CA3}$, $Corr_{CA1}$, and $Corr_{EC}$, respectively (see Figs. \ref{Fig_3} and \ref{Fig_6}).

\subsubsection{Pattern Completion}
Pattern completion is defined as the retrieval of additional information from a memory network that was not present in the recall cue. To measure pattern completion in our model, we compare the retrieval quality at some stage $Corr(a_{l, m},\tilde{a}_{l, m})$ to the retrieval quality at the next stage $Corr(b_{l, m},\tilde{b}_{l, m})$. Here, the stages correspond either to two connected layers in a feedforward network, or to subsequent network states in the recurrent CA3 network. 

If the first layer is the input pattern in EC, then we use the recall cue quality instead of the retrieval quality(!!!!).
%
To perform the comparison, we make a scatter plot of
$Corr(b_{l, m},\tilde{b}_{l, m})$ and
$Corr(a_{l, m},\tilde{a}_{l, m})$ 
for all pairs of stored and retrieved patterns.
If the points line up along the identity line, then the processing does not add any information and thus does not perform pattern completion. Points above the main diagonal show that the output of the network is more similar to the stored pattern than the input was. So the network has performed some amount of pattern completion. The more the measurements are above the diagonal, the better the pattern completion performance (see Fig. \ref{Fig_3}). Measurements below the main diagonal indicate that the output of the network is on average less similar to the stored pattern than the input was, reflecting that information was lost during processing (see Fig. \ref{Fig_3}, RCN model).

To quantify the degree of pattern completion in a processing step, we define the pattern completion index (PCI) as the area between the main diagonal and the averaged output retrieval quality. Averaging was performed in 10 bins in input retrieval quality. The area is multiplied by a factor of 2 to obtain numerical values of the PCI between -1 and 1. Positive values imply that the network performs pattern completion, whereas negative values show that the network loses information. Values close to zero imply that the processing step does neither. 

\subsubsection*{Sequence Memory Capacity}

We further study the capacity of the network and estimate the number of patterns (sequences) the network is able to store and retrieve. For the specific number of stored sequences, we calculated the pattern completion index (PCI) for different projections (Figs.~\ref{Fig_5} and \ref{Fig_8}). 
We define the network capacity in our model as the maximum number of sequences than can be stored in the network such that this PCI $> 0$.

\subsubsection*{Robustness Against Dynamic Noise}

To make our neuron model more biologically plausible, we also add noise to the neural dynamics. Eq.~\ref{activation} is rewritten as
\begin{align}
\label{dynamic-noise}
h^i = \sum_{j=1}^{N_\mathrm{in}} w^{ij}u^j + \xi ^i (0,\sigma),
\end{align}  
where $\xi^i$ is independent Gaussian noise with zero mean and variance $\sigma$. The noise term is present in the dynamics both during storage and retrieval. We exclude this term for EC input neurons to control the amount of noise added to the recall cue in the retrieval phase.
%So we obtain only one PCI for the EC-CA3 projection. Since the range of input activations is quite different, adding the same noise to all layers would have larger effects in layers with low activations. To ensure that the noise is comparable in all layers, we normalize $ \sigma $ by the average pattern activity of the corresponding layer $\langle h \rangle$. 


% Results and Discussion can be combined.
\section{Results}
\subsection{Sequences Completion in the CA3 Network}

The CRISP hypotheses concerning the operation of hippocampal circuit is based on the notion that the CA3 network operates as a heteroautoassociative memory. It is able, when provided with a noisy cue, to selectively retrieve a specific sequence of activity patterns from among several stored on the same synaptic strengths. This property is due to the architecture of the CA3 network which is established during training. 

We first investigate the sensitivity of the three different network models of CA3 (RCN, LCN, and DDN($\alpha$)) to noise. Fig.~\ref{Fig_3}A shows the performance of the different networks in retrieving the stored sequences for the cue quality $u'_{l, 1} = 0.4$, which is initialised in EC input. In the subplots, each line indicates the retrieval quality for one sequence as a function of the time within the sequence. Our results show that overall the RCN is highly sensitive to noise as the retrieval quality degrades quickly.  
%We test even when retrieval is initiated with high quality cues ...
By contrasts, the DDN($\alpha \lesssim .8$) reproduces the entire sequence almost perfectly. It is thus able to both perform pattern completion and reach a retrieval quality of 1, even when retrieval is initiated with highly corrupted cues via EC, $C_{CA3} (y_1, \tilde{y}_1) \simeq .3 $. 
However, the DDN($\alpha \gtrsim .8$) is quite a bit more sensitive and does not maintain the retrieval quality for extended stretches of all sequences. When $\alpha$ approaches its maximum value, the network shares the temporal correlations in CA3 activity due to the correlated grid inputs from EC and the network performance decreases abruptly. The extreme case is when the patterns in the CA3 during the learning phase are driven purely by EC inputs, $\alpha = 1$. In this case, the network model cannot even retrieve one of the stored sequences.
% 

Next, we test the LCN, which allows controlling the temporal correlation between the stored patterns. This is able to generate a continuously moving bump of activity, performs moderate pattern completion and maintains the retrieval quality for the remainder of the sequence (Fig.~\ref{Fig_3}A, last panel).
In this example, the adaptation parameter is $J = 0.33$. The slower the bump moves, the more robust the network is (data not shown).
%By contrast LCN, which is able to generate a continuously moving bump of activity, performs moderate pattern completion and maintains the retrieval quality for the remainder of the sequence
 
The results for these networks are summarized, for the whole range of noise-levels, $u'_{l, 1} \in  { 0, .2, .4, .6, .8, 1 } $, in the PCI plots for the CA3 dynamics shown in Fig.~\ref{Fig_3}B. Since the data points for the RCN are below the diagonal the network as a whole does not perform sequence completion (PCI = -0.1??).
% 
The DDN($\alpha \lesssim 0.8 $), performs sequence completion as the data points are well above the diagonal (PCI = 0.26, put the numbers inside the figure???). For the DDN($\alpha \gtrsim 0.8 $), sequence completion is good when there is less overlap between the stored sequences. Otherwise, the network fails to complete the sequence, since data points lie on the diagonal and below (PCI = 0.09).  
%
\begin{figure}[!htb]
\centering\includegraphics[width=.7\linewidth]{Fig_3.eps}
\caption{\textbf{A:} Example of recall performance in CA3. 
Each panel shows the retrieval quality in a CA3 model as labelled on the right hand side of the figure, when recall cue with cue quality $C_{EC}(u_1, u'_1) = 0.4$ is provided to EC.
The horizontal axes represents the position of the pattern in the sequence or time-step. Each colored line shows the correlation between the retrieved and the corresponding stored patterns in a sequence.
The RCN is highly sensitive to noise, whereas the LCN (last panel) seems to keep the same input information while retrieving the stored sequence. 
The DDN($\alpha \lesssim .8$) shows the best performance and retrieves the previously stored sequence from the noisy input cue. The ($\alpha \gtrsim .8$), depending on the amount of correlation between the stored patterns, either completes the sequence to the correct one or confuses and fails the sequence completion.
\textbf{B:}~Pattern completion plot. It shows the summary of Retrieval quality in CA3-CA3 projections for the entire range of noise levels.
To include the whole range of noise-levels, the pattern completion plot for different network models is shown (see Methods and Materials). Data-points above the diagonal stand for good sequence completion, whereas data below the diagonal show that information in the input is lost. Data on the diagonal show that the network maintains the information in the input cue along the sequence. Overall, the DDN($\alpha \lesssim .8$) performs best, even with highly corrupted cues. 
%The PCI value for the RCN, LCN, PTR and PTG models are $-0.5, 0.1, 0.56$ and $0.09$, respectively.
 }
\label{Fig_3}
\end{figure}
%
For the LCN, with low moving bump speed $J = 0.33$ data-points lie mostly on the diagonal meaning that the network model maintains the information from the input cue and sometimes performs sequence completion (PCI = 0.01). 
%
To conclude, the performance of recurrent networks in generating robust spatio-temporal sequences highly depends on their structure. These results show that the DDN can perform sequence completion when the $\alpha \lesssim 0.8$. Since the CA3 patterns during the learning are partly driven by EC grid inputs, therefore the transferred correlation between the stored patterns might play an important role in retrieving the stored sequences. 
Thus, to study the dynamics of sequence retrieval for DDN in more details and the requirement of the inputs to CA3 nodes for efficient storage of pattern sequences, we calculated the average temporal correlation $ <<?>>$ between stored patterns against the parameter $\alpha$ (Fig.~\ref{Fig_4}).   

\begin{figure}[!htb]
\centering\includegraphics[width=1.\linewidth]{Fig_4.eps}
\caption{\textbf{Average temporal correlation.} 
The blue data points show the PCI value, which is calculated based on the results in Fig. \ref{Fig_3}B, for different $\alpha$ parameters. The red data points show the corresponding average temporal correlation between the stored pattern sequences.
}
\label{Fig_4}
\end{figure}
%we calculated the correlation between each retrieved pattern and all the stored patterns of the sequences for each network (Fig.~\ref{Fig_4}).
We indeed find that the amount of pattern completion in CA3 (blue) is strongly related to the correlation between the stored patterns (red). 
For DDN($\alpha \lesssim .8$), the average temporal correlation between the stored patterns of the sequences is close to zero. 

By calculating the amount of correlation that would end up being imposed on CA3 firing pattern produced solely by the EC input and by the effect of the recurrent connections, we have been able to show that an input of the EC, alone, is unable to direct efficient sequence storage. Such an input induces correlation between stored patterns and it turns out that driving the dynamics of the network slightly by the randomizing effect of the recurrent collaterals, washes out this correlation. This is the manifestation, in the CA3 network, of a general problem affecting memory storage. 
Therefore, under the same conditions, the robustness of the dynamics of a DDN network depends highly on the amount of correlation between the stored patterns. When the average temporal correlation is very close to zero, the initialized network state is attracted to the correct stored sequences. The overlap among the stored sequences is insignificant so that the network can perform perfect sequence completion.

%Further calculations (data not schown) reveal that even if the recall cue quality is close to zero ( $\lesssim \: 0.15$) this network can retrieve one of the stored sequences, but not necessarily the correct one. However, in the case of retrieving the correct sequence the retrieved sequence approaches the stored sequence very slowly. For example, in Fig.~\ref{Fig_3}A when the recall cue quality is $0.05$, one of the retrieved sequences matches the stored one in the fourth time step, whereas with much higher cue qualities it matches in the second time step.    
%
For the DDN($\alpha \gtrsim .8$), due to the underlying grid input patterns from EC, some elements in the sequences are correlated to some extent, and these correlations deteriorate the network performance.

%Furthermore, there is no correlation between the retrieved patterns and all the other stored patterns, since the network generates a large number of uncorrelated patterns. 

Since the LCN structure is restricted and cannot generate a sufficient number of uncorrelated patterns, there is clear and relatively large temporal overlap between the stored patterns. 

Whether the overlap is harmful for sequence completion and pattern hetero-association in feedforward networks will be analysed in the next section. 

In summary, we find that the robustness of sequence generation depends sensitively on the network architecture. The RCN is very sensitive to noise, whereas the LCN generates moderately robust neural sequences. The DDN performs perfect sequence completion as long as the correlation between the stored patterns of the sequences is zero, otherwise the retrieved sequence deviates from its original stored one. The DDN($\alpha \lesssim .8$) exhibits the most robust dynamics since the stored pattern sequences are orthogonal to each other.

The contribution of the intrinsic dynamics facilitates the episode memory storage in the CA3 network. The EC input solely would not produce suitable activity patterns in CA3 that contains sufficient orthogonality for sequence learning.


Predictions arising from the analysis of CA3: Given the results above, that the intrinsic dynamics is important during storage of pattern sequences in the hippocampus, we have shown that purely driving the CA3 dynamics by EC inputs should result, in catastrophic interference in CA3. In contrast, allowing only about 10$\%$ of the CA3 activity to be driven by the intrinsic dynamics, result in a proper retrieval of the stored sequences in CA3. The performance in retrieval would be due to the orthogonality that is induced to the stored patterns by intrinsic dynamics. 

When modelling CA3 as a RCN, the network fails to retrieve the stored sequences in CA3. Since the sequence completion is crucial for the complete loop performance, we exclude this model for the rest of the analysis.
%The RCN fail to complete the sequence in CA3 and therefore the CA3-CA1 and CA1-EC projections cannot recover the sequence anymore, except for the first pattern.


\begin{figure}[!htb]
\centering\includegraphics[width=1.\linewidth]{Fig_5.eps}
\caption{\textbf{Capacity analysis.} 
\textbf{A}:~Recall performance as quantified by the pattern completion index (PCI) in CA3-CA3 projections, for the four different DDN networks as a function of the number of stored sequences. 
%The grey shading of the marker indicates the range of the data points between zero and one that is used to calculate the PCI (e.g., see Fig.~\ref{Fig_8}). White indicates a complete range, black a single data point near zero. The range is important since a small range implies that the PCI is unreliable.
We define the CA3 capacity in our model as the maximum number of sequences than can be stored such that PCI $> 0$. \textbf{B}:~Robustness against dynamic noise.
Same plotting convention as in \textbf{A}.
Performance of four different DDN networks at different levels of dynamic noise are shown.
}.   
\label{Fig_5}
\end{figure}

\subsubsection{Sequence Memory Capacity and the Effect of Dynamic Noise on CA3 Network}

We study the CA3 capacity by calculating the pattern completion index (PCI) for DDN network. Our results show no evidence for an abrupt change in retrieval quality, that would be evidence for catastrophic interference as more and more patterns are stored. Instead, retrieval quality degrades gracefully in all processing stages and for DDN($\alpha \lesssim .8$).
The DDN($\alpha \gtrsim .9$) does not reach our criterion (PCI > 0) at all and thus have zero capacity. The DDN, with  model has a capacity of around 50 sequences (about 800 patterns). The model with $\alpha = ?$ in CA3 shows the highest capacity of around 60 sequences (about 900 patterns). The most important reason is that CA3 dynamics has high capacity (Fig.~\ref{Fig_5}).  
The model with DDN, =0.6 in CA3 still shows better performance relative to the others, but the network capacity is lower (about 400 patterns) than for random input

We have found in general that the maximum number of pattern sequences that can be retrieved ($\simeq 1000$), is compatible with the previous studies. Treves and Rolls (1994) have shown that the recurrent network capacity is proportional to the number of modifiable synapses per cell, by a factor that increases roughly with the inverse of the pattern sparsity (cite!). 
%

In Fig.~\ref{Fig_5}B, we show the results of the effect of the noise dynamic on CA3 network performance. Each color depicts the results for the corresponding CA3 network model and values show the amount of pattern completion in CA3.
Furthermore, we get the same PCI for the CA3-CA3 projection in each simulation.
Comparing the performance of the networks confirms that the networks with >0.8 in CA3 are robust against noise in the dynamics to some extent (up to $\sigma \simeq 0.4$). Even when $ 40 \% $ noise of its average activity pattern is added to the CA3 dynamic, it completes the sequence successfully. In this simulation we store 256 patterns (16 sequences) in the network. The network with $\alpha = 1$ is shown as reference for the other networks!    

A number of other factors might influence the network capacity. One such factor is the sparsity of the connectivity. However, when we performed simulations with all-to-all connectivity (data not shown), we found no qualitative differences in the results, indicating that our results are not sensitive to the number of synapses within the range tested here. 

\begin{figure}[!htb]
\centering\includegraphics[width=1.\linewidth]{Fig_6.eps}
\caption{\textbf{Comparison of average retrieval performance at different processing stages.}
Each panel shows the result for a different CA3 model. X-axes illustrates the pattern completion in the EC-CA3, CA3-CA3 (through time), CA3-CA1 and CA1-EC projections. Each data point, except for the retrieval cue, shows the average performance in the indicated step. Different colors indicate the performance in different time steps.}
\label{Fig_6}
\end{figure}

\begin{figure}[!htb]
\centering\includegraphics[width=1.\linewidth]{Fig_7.eps}
\caption{\textbf{Summary of pattern and sequence completion.} In this figure the pattern completion is shown by comparing the first and last retrieval correlations of the sequences in EC. Each panel correspond to a  DDN model with different $\alpha$. The last panel (down left) illustrates the PCI value against the parameter $\alpha$.}
\label{Fig_7}
\end{figure}


\begin{figure}[!htb]
\centering\includegraphics[width=1.\linewidth]{Fig_8.eps}
\caption{\textbf{Capacity analysis.} 
\textbf{A}:~Recall performance as quantified by the pattern completion index (PCI) by comparing the first and last retrieval correlations of the sequences in EC, for the five different DDN networks as a function of the number of stored sequences. 
%The grey shading of the marker indicates the range of the data points between zero and one that is used to calculate the PCI (e.g., see Fig.~\ref{Fig_8}). White indicates a complete range, black a single data point near zero. The range is important since a small range implies that the PCI is unreliable.
\textbf{B}:~Effect of noise in neural dynamics on overall network performance.
Same plotting convention as in \textbf{A}.
}.   
\label{Fig_8}
\end{figure}


\subsection{Storing and Retrieving Pattern Sequences in the Hippocampal Model}

%Others, such as Marr (1971), have noted that cued retrieval of one of a set of stored firing patterns can also be achieved with an alternative architecture, consisting of a cas- cade of feedforward associative nets (Willshaw et al., 1969), possibly assisted to some minor extent by the effect of recur- rent collaterals (see Willshaw and Buckingham, 1990).



To test the important features of the CRISP hypothesis that CA3 performs sequence completion and the feedforward projections perform pattern completion, we compared a simulation of the complete network EC-CA3-CA1-EC with different CA3 network models.
%
Grid patterns in EC are spatially correlated, which affects the feedforward hetero-association and thus the overall model performance. 
%
In order to study the temporal evolution of the pattern completion in feedforward projections and the sequence completion in CA3 concurrently,  we perform simulations where the retrieval cues are presented in the EC and averaged over the retrieval qualities in each time step, $<C(a_t, \tilde{a}_t)>_l$.
%
Fig.~\ref{Fig_6} demonstrates the results for the first 8 time steps in the different projections in different processing steps, namely CA3, CA1 and EC. In all networks the cue quality is $C_{EC}(u_1, u'_1) = 0.4$.
%
Each panel illustrates the results for a different DDN($\alpha$) model. As a control example, in terms of the correlation between the stored patterns, we show the results for the LCN model as well (last panel). 
%

%The major difference between the network performance in the two different input scenarios is revealed when modeling CA3 as a IDN. With grid input, IDN mostly fails to complete the sequences because of the correlation between the stored patterns in CA3 (Fig.~\ref{Fig_7}, fourth column). The patterns in CA3 are directly triggered by EC, which are correlated. However, some of the sequences that have less correlation with other sequences are retrieved to some extent. Therefore, these sequences are mostly completed while transferring from CA3 back to EC.Comparing the network performances with different DDN models in CA3 indicates some differences in the degree of pattern completion in EC-CA3 projections.

The overall memory performance on correlated grid input is best using the DDN($\alpha \lesssim .8$). 
%
 However, it is intriguing that not all processing steps perform better. 
 %
In particular, the first step of hetero-association in EC-CA3 using the DDN($\alpha \lesssim .8$) performs worse than in the DDN($\alpha \gtrsim .8$). The most obvious case is when comparing the DDN(0) with  the DDN(1).
%
The DDN(0) loses the information in the input cue, whereas DDN(1) performs a small amount of pattern completion.
%
As larger the parameter $\alpha$, the better the performance of the EC-CA3 at pattern completion.
%
Overall, the network performs only a small amount of pattern completion through the EC-CA3 projection, due to the correlation between the patterns in EC.
%

The next two feedforward steps show the pattern completion through the CA3-CA1 and CA1-EC projections. With the DDN model, both steps perform pattern completion, however, it only occurs for the initial patterns of the sequences (t = 1, 2 and 3). When $\alpha < 0.8$, except for the initial patterns the rest of the sequence is already completed in CA3. This is apparent in Fig.~\ref{Fig_3}A, as well.
%
For t > 3, the information from the input cues is preserved for both projections.
%

With LCN, the CA3-CA1 projection, except for the first pattern, almost loses all information and the CA1-EC projection preserves the information.
Since the LCN maintains the information of the input cue along the sequence in CA3 (see Fig.\% \ref{Fig_3}A), one would expect pattern completion through the CA3-CA1 and CA1-EC projections. Surprisingly, pattern completion fails in this model, too. This is because patterns within and between the sequences are highly correlated. The correlations, which depend on the moving bump speed parameter $J$, introduce an overlap between the weights that the CA3-CA1 projection uses to hetero-associate the patterns, and this in turn impairs the network in retrieving the correct patterns in CA1.
%
In principle, the CA3-CA1 and CA1-EC projections are identical in all networks, nevertheless their performances depend on the correlation between the patterns that are generated in CA3 during retrieval. With the LCN model, CA3 generates highly correlated patterns, therefore the CA3-CA1 and CA1-EC projections can not decode the stored information. Whereas the DDN($\alpha \lesssim .8$) models generate uncorrelated patterns in CA3, which allow for better pattern completion. When DDN($\alpha \gtrsim .8$) models are used, there is correlation between the retrieved patterns and that the CA3-CA1 and CA1-EC projections performs less pattern completion (see Fig.\% \ref{Fig_3}A).

%
Fig.~\ref{Fig_7} summarises the overall performance of the network for the whole range of noise-levels. We compared the recall cue quality to the last pattern of the retrieved sequence in EC.
%
The last panel (down left) shows the PCI against $\alpha$.
%
The models with the DDN($\alpha \gtrsim .8$) in CA3 fails to retrieve the stored sequences at any recall cue quality. Whereas with the DDN($\alpha \lesssim .8$) in CA3, the model performs perfect sequence completion for the recall cue qualities greater than about $0.2$.
%
The reason is that the correlation between the patterns in EC is transferred to CA3 and that the CA3 dynamics then fails to retrieve most of the sequences. This implies that CA3 weights should be trained partly independently of EC. Furthermore, the temporal correlation between stored patterns is detrimental for pattern completion in feedforward networks. 
To conclude, the CA3 dynamics is essential for the recall of stored sequences and must generate robust and at the same time uncorrelated pattern sequences.          

\subsubsection{Sequence Memory Capacity in the Complete Loop}

We further study the capacity of the network at the output stage based on the results in Fig.~\ref{Fig_7} . 
The best measure of overall memory performance is arguably the comparison of last retrieved pattern to the recall cue, which is an indicator for sequence completion.
%
For the specific number of stored sequences, we calculated the pattern completion index (PCI) and define the network capacity in our model as the maximum number of sequences than can be stored in the network such that this PCI $> 0$.
%
As indicated in the Fig.~\ref{Fig_8}A, the retrieval quality degrades gracefully in all DDN($\alpha \lesssim .8$)  models.
% 
The DDN models, with $\alpha = .0, .4$ have a capacity of around 25 sequences (about 400 patterns).
%
The model with $\alpha = .8$ in CA3 shows the highest capacity of around 50 sequences (about 800 patterns).
%
The most important reason is that CA3 dynamics has high capacity (Fig.~\ref{Fig_5}A) and at the same time generates orthogonal patterns, which helps in hetero-association.  
%
As van be predicted from the previous figure, the DDN(1) has the capacity zero. 

\subsubsection{Effect of Dynamic Noise on Network Performance}
Furthermore, we test the impact of the dynamic noise on the network capacity. We use the same criteria as in Fig.~\ref{Fig_8}A for determining the capacity of the network. In this simulation we store 256 patterns (16 sequences) in the network. 
%
In Fig.~\ref{Fig_8}B, each color depicts the capacity of the corresponding CA3 network against dynamic noise.
%  
Comparing the net performance of the networks confirms that the DDN($\alpha \lesssim 0.8$) are robust against noise in the dynamics (up to $\sigma \simeq 0.4$). 
%
Even when $ 40 \% $ noise of its average activity pattern is added to the CA3 dynamic, it completes the sequence successfully. 
%
The data indicate that in the presence of dynamic noise the DDN(.8) performs best. 
%
However, the sensitivity to noise increases abruptly for all networks in the presence of more noise.
%
As we expected the DDN(1) is not robust against noise at all.
\begin{figure}[!htb]
\centering\includegraphics[width=.8\linewidth]{Fig_9.eps}
\caption{\textbf{Dimensionality of the pattern manifold in different layers.} PCA of the patterns stored in EC, CA1 and CA3 for different network models. The plots show the number of components that are required to account for a total of $85 \%$ of the variance. The top row shows the fraction of the variance explained by individual components, the bottom row shows the cumulative fraction. The manifold of patterns in EC with grid input has very low dimensionality. Since the grid patterns are generated from moving in a 2-d space, the true dimensionality is 2, but the manifold is highly non-linear and PCA cannot extract this information. A similar effect can be seen in CA1 and CA3 with DDN(1), since these areas are directly driven by EC. CA3 patterns in a LCN, are low dimensional since network activity is constrained by the attractor dynamics to a 2-d manifold. The DDN(0) patterns has the highest dimentionality, since the patterns are purly generated by the random recurrent connections in CA3 and are nearly orthogonal to each other. For the DDN($\alpha \lesssim 0.8$), the dimentionality is still close to maximum. As the $\alpha$ approaches its maximum value, the dimentionality of the patterns suddenly drops toward that of in DDN(1). These results are compatible with the results of the average temporal correlations in Fig.\: \ref{Fig_4} }
\label{Fig_9}
\end{figure}


\begin{figure}[!htb]
\centering\includegraphics[width=1.\linewidth]{Fig_10.eps}
\caption{\textbf{How close are retrieved CA1 patterns to the correct one?} Histograms of correlations between retrieved patterns and corresponding stored patterns (cyan) and between retrieved patterns and all other stored patterns (red). Retrieval is initiated with the perfect recall cues. The number inside each panel shows the confusion rate. That is how often the correlation between the retrieved and the original pattern is smaller than the correlation between retrieved pattern and at least one of the other stored patterns in the network.} 
\label{Fig_10}
\end{figure}


\subsection{Dimensionality and Pairwise Correlation Analysis}

A different input system is needed to trigger retrieval
A heteroassociative memory network needs uncorrelated patterns in the input layer, so that, it performs proper pattern completion when it retrieves a stored pattern.
However, we have shown that the performance of the EC-CA3 projections are in the case of grid inputs slightly different and in turn it can affects the CA3 performance, motivating the necessity of further analysis. 



%Finally we come to Principal Components Analysis (PCA). What is it? It is a way of identifying patterns in data, and expressing the data in such a way as to highlight their similarities and differences. Since patterns in data can be hard to find in data of high dimension, where the luxury of graphical representation is not available, PCA is a powerful tool for analysing data. The other main advantage of PCA is that once you have found these patterns in the data, and you compress the data, ie. by reducing the number of dimensions, without much loss of information. This technique used in image compression, as we will see in a later section.


%Further calculations (data not schown) reveal that even if the recall cue quality is close to zero ( $\lesssim \: 0.15$) the DDN network can retrieve one of the stored sequences, but not necessarily the correct one. However, in the case of retrieving the correct sequence the retrieved sequence approaches the stored sequence very slowly. For example, in Fig.~\ref{Fig_3}A when the recall cue quality is $0.05$, one of the retrieved sequences matches the stored one in the fourth time step, whereas with much higher cue qualities it matches in the second time step.
To better understand why this occurs and how the network recovers from this apparent loss of information in later processing steps, we investigated the manifolds on which the stored patterns in EC, CA3, and CA1 lie. The dimensionality of the pattern space in each layer equals the number of cells $N$. Within this space, the manifold spanned by the $P$ stored patterns has at most a dimensionality of $P$ (=256). However, due to the correlation between the stored patterns the effective dimensionality of the manifold can be much smaller. To determine this dimensionality, we apply principal component analysis (PCA) to the stored patterns in each layer. PCA finds the dimensions (or components) that explain most of the variance of the given data \cite[chapter 4]{Hastie2009}. 

Now we address the differences in the level of pattern completion in the network. (\RN{1}): Why does pattern completion in the EC-CA3 projection work better when $\alpha$ is lager? Fig.~\ref{Fig_9} illustrates the PCA analysis on the stored patterns. In the case of grid inputs, only 20 dimensions explain about $85 \%$ of the variance, therefore EC patterns share their weights when hetero-association is performed between EC and CA3. By contrast, for DDN(0), the patterns are expanded in about 200 dimensions and are almost orthogonal. (\RN{2}): 
However, with LCN, the EC-CA3 projection occasionally performs better or worst than the other models, since the vertical scatter for the LCN network is slightly larger. The PCA analysis on the patterns stored in CA3 shows that stored patterns in the DDN(==1) models are expanded in a higher dimensional space ($\sim$ 200) compared to patterns stored in LCN ($csim$ 20) (Fig.~\ref{Fig_9}). Therefore, in the LCN, the retrieved patterns lie in a lower dimensional space and therefore are closer to the stored ones. Consequently, the reconstruction error is lower. At the same time, the higher overlap between the patterns in CA3 hurts pattern completion in the EC-CA3 projection. (\RN{3}): With grid inputs the EC-CA3 projection shows slightly better performance for DDN with larger Alpha (see Fig.~\ref{Fig_6}). The PCA analysis on the stored patterns in CA3 shows that stored patterns in the DDN with larger Alpha span a slightly lower dimensional space than patterns in the DDN with smaller Alpha (Fig.~\ref{Fig_9}). Therefore, the retrieved patterns lie in a lower dimensional space, which keeps the retrieval error small.

In brief, the argument is based on the notion that the dimentionality in EC is relatively low, that is, only 20 dimensions explain about $85 \%$ of the variance, and in CA3 it depends on the parameter $\alpha$. In order not to lose too much information in EC-CA3 projections, but rather perform pattern completion, it turns out that the dimentionality in the CA3 network should be as much as possible close to that dimentionality in EC. For example, for DDN when Alpha is close to one, the EC-CA3 projections performs better and can then initiate the CA3 dynamics with the better retrieval cue. In contrast, CA3 needs uncorrelated patterns during training to retrieve the full sequences.  Therefore, we need to trade off between these two requirements. This is expressed by the results that give the specific range for Alpha that can optimise the network capacity. alpha around .8. 
%

The compression to a low dimentional space has negative consequences for decoding CA3 patterns downstream in the CA3-CA1 projection. For instance, the LCN completes the sequences moderately, but the information cannot be decoded from CA3 to CA1 due to the highly correlated patterns in CA3. In the other CA3 models, the CA3-CA1 projection can perform pattern completion if the sequences are completed moderately in CA3 (see Fig.~\ref{Fig_6}). Finally, to highlight that the network performs best if when the CA3 dynamics is robust and at the same time generates non-correlated patterns, we compared the correlations between retrieved patterns and their corresponding original versions as well as between the retrieved patterns and all other stored patterns in CA1. If the pattern is remembered correctly, the correlations between the retrieved patterns and their original version should be distinct from the correlations between the retrieved patterns and all other stored ones. Fig.~\ref{Fig_10} illustrates the pairwise correlation analysis in CA1. The retrieval is initiated with the perfect recall cue in EC. For DDN(1), the two distributions are rather wide with low mean and largely overlap with each other. The overlap is further quantified by the confusion rate, i.e., how often the correlation between the retrieved and the original pattern is smaller than the correlation between retrieved pattern and at least one of the other stored patterns in the network.

However, for grid inputs in the case of DDN(1), the correlations are low and they overlap with each other, meaning that correlated patterns deteriorate robustness of CA3 as well as the feed-forward hetero-association.

For the DDN(<.8), the correlations between the retrieved patterns and their original version are high and distinct from the other distribution.     

\section{Discussion}

We have investigated the storage and retrieval of memory sequences in the hippocampal circuit based on the recently proposed CRISP theory. CRISP is based on intrinsic sequences in CA3, and pattern completion in feedforward projections. We found that the performance of the network model that implements CRISP critically depends on the CA3 dynamics in generating robust sequences. Furthermore, the correlations between different stored patterns deteriorate pattern completion in feedforward networks. For instance when modeling CA3 as a LCN, stored sequence are retrieved robustly within CA3, but the  correlations between patterns within a sequence impair pattern completion when decoding CA3 patterns. In contrast to LCN, a RCN facilitates decoding, but sequence retrieval fails if any noise is present.
%
It turns out that the DDN model for the CA3 dynamics, that generates mutually uncorrelated and robust memory sequences of activity patterns, has the best memory performance and 
%
Including only about $20\%$ of the intrinsic CA3 dynamics is sufficient to remove the grid correlations between the EC input patterns and allow an efficient sequence memory storage.   
%

Intrinsically generated sequences have been observed in a number of different studies. During the delay period in an ongoing task, hippocampal neurons fire in a reproducible temporal sequence \cite{pastalkova2008internally, macdonald2011hippocampal}.  Sequential activities were observed in an offline state before rodents explore a novel environment, which were correlated with the ordering of place fields in the novel environment (preplay) \cite{dragoi2011preplay}. This preplay phenomenon suggests that the offline sequences could not have been established by external sensory inputs and are intrinsic to CA3 \cite{azizi2013computational}. These observations are difficult to reconcile with the standard framework. The pool of intrinsic sequences in CA3 might be established during development and/or during extended rest periods.  


\subsection{Comparison of CRISP to the Standard Framework}


%Firstly, neither plasticity in CA3 nor CA3 itself seems necessary for memory storage. Animals with CA3 impairment can successfully retrieve the goal location when all training cues are available \cite{nakazawa2002requirement, gold2005role, fellini2009pharmacological}. On the other hand, animals with complete hippocampal lesions show learning deficits even when all the cues are available \cite{gilbert1998memory, morris1982place}. Taken together, these results suggest that CA3 is not the actual place of memory storage, which must occur in parts of the hippocampus outside of CA3 \cite{cheng2013crisp}. 

Finally, the standard framework mostly focuses on an isolated CA3 network and neglects the inevitable encoding and decoding in feedforward projections. It has been shown computationally that hetero-associative projections are capable of reconstructing the memory of grid cell patterns even when the recurrent connections (auto-associative function) in CA3 are removed \cite{neher2015memory}. This study illustrates how essential it is to consider the whole hippocampal loop while investigating individual functional roles of the subregions. 

Here we can talk about the effect of the EC input on the CA3 dynamics during learning. In standard framework the inputs to EC are drived only via DG!!!1 

In summary, even though the standard framework has been influential in explaining the neural mechanisms of episodic memory storage, the evidence against it is mounting, warranting an alternative to an attractor network in CA3. CRISP suggests that the feedforward connectivity between hippocampal sublayers act as a feedforward pattern association network that is more important than the recurrent CA3-CA3 association system. 



\subsection{The Function of DG}

In our simulation, we did not include explicitly the function of DG. However, the influence of DG can be integrated into our model. A number of studies have indicated that DG orthogonalizes the patterns before storage, a process known as pattern separation \cite{mcnaughton1987hippocampal, o1994hippocampal, marr1991simple, treves2008mammalian}. This process is facilitated by adult neurogenesis in DG, a process in which new neurons (granule cells) continue to be generated and incorporated into the network. New born cells have little overlap with older DG cells with respect to their projections to CA3 \cite{becker2005computational, wiskott2006functional, aimone2009computational}. Our results illustrate that the memory performance is best if the CA3 network is pre-trained on sequences of random patterns. Orthogonal CA3 patterns are good for memory performance and precisely what one would expect if DG performs perfect pattern separation. With an IDN in CA3, the linear transformation of the patterns from EC to CA3 keeps the correlations between grid code patterns in CA3, which subsequently deteriorates the network performance. Here, we suggest that including the DG layer might be intermediate between the PTR and IDN models and that the balance might shift throughout the animals life time. 

The rate of neurogenesis has been found to decline dramatically with age \cite{ kuhn1996neurogenesis, klempin2007adult}. Comparing mice in middle age to early adulthood, older animals have about $80\%$ fewer neural progenitor cell proliferation, neuronal differentiation, and newborn neuron survival \cite{kuipers2015changes}. In the mouse DG, only $8.5\%$ of the neurons born postnatally are added after middle age \cite{lazic2012modeling}. We propose that a pool of uncorrelated sequences is established in the CA3 network when newborn neurons integrate into the DG network and provide orthogonal activity to CA3. In our model, this corresponds to the pre-training of the CA3 network on sequences of random patterns. Memory storage during early adulthood would make extensive use of these random sequences. Since the rate of newborn granule cells in DG is substantially lower in middle ages than during early adulthood, the generation of random CA3 patterns would become less prevalent. So, the IDN might be a more plausible model of CA3 in middle age. We did not model such a switch here since our focus was on the performance of the different CA3 dynamics.


\subsection{Pattern Completion in CA1}
Another important issue is the contribution of CA1 in episodic memory storage. The standard framework does not offer a clear function for CA1, but some studies hypothesized that CA1 plays a role in novelty or mismatch detection \cite{hasselmo1996encoding, lisman2001storage}. CA1 might detect novelty by increasing its activity when rats are exposed to novel environments \cite{karlsson2008network, csicsvari2007place}.
%
Lesions to CA1 produce deficits in the retrieval of contextual fear conditioning \cite{lee2004differential}, and retrieval of spatial information when learning a Hebb-Williams maze \cite{jerman2006disconnection, vago2007role, hunsaker2008double}. 
%
An alternative function of CA1 supported by our results, is pattern completion of CA3 patterns to increases the precision and robustness of retrieval (see Fig. \ref{Fig_5} and \ref{Fig_7}). Hence, CA1 decodes the highly transformed patterns in CA3 back to their original versions in EC \cite{neher2015memory}.


\subsection{Relationship to Spatial Memory}
In our model, we compare the storage of random patterns to the storage of grid patterns. The latter is an example of spatial memory and likely an ethologically relevant function of the hippocampus. It has been shown that the  hippocampus is necessary for spatial learning in rodents \cite{morris1982place} and humans \cite{burgess2002human}.  However, several other types of cells have been discovered in the hippocampal formation as well, including head direction cells \cite{taube1990head}, border cells \cite{solstad2008representation}, odor-sensitive cells \cite{deshmukh2003representation}, irregular spatial cells or nonspatial cells \cite{zhang2013optogenetic}, and time cells \cite{macdonald2011hippocampal, salz2016time}. This diversity of cell types is consistent with the function of the human hippocampus in episodic memory \cite{burgess2002human}. While the focus of this article was on episodic memory, our network stored spatial information from grid cells. The full range of inputs to the hippocampus and the mixture of different inputs are poorly explored. We expect that our results are applicable beyond grid patterns because it is the correlation between CA3 patterns that are detrimental to memory performance and these correlations are present in any of the aforementioned cell types and quite likely in episodic memory patterns in general.

Our model could help to study whether the spatial representation in CA1 and CA3 can be reconciled with episodic memory in the same neural network model. We found previously that a fairly generic solution to the transformation from grid cells to place cells could be learned in a feedforward model \cite{cheng2011structure}. We also found evidence for spatial coding in CA1 and CA3 in a model related to the current one \cite{neher2015memory}. Since our model includes the hippocampal circuit, it enables future investigation of spatial representations in the hippocampal subregions. 


\subsection{Dynamics and the temporal dimension}  

One of the concerns is how the hippocampus associates the events that are seperated by more than a second, while with the continuous dynamics, that are present in the brain, any attractor (in CA3) finds the basin of attraction within 10-20 ms. This time scale of activity propagation in continious attractors depends crucially on the time constants governing synaptic conductances (Battaglia and Treves 1998; Treves 1993; Panzeri et al 2001). 
%
Various oscilatory rhythms (e.g. theta, gamma, ripples) are assumed to be essential for episodic memory and sequence learning in hippocampus.(Traub et al, Rev Neurosci 13:1-30, 2002).
%
We assume that there is an external mechanism that acting on the hippocampus and which is responsible for synchronising the activity through the CA3 dynamics and the entire network. There could be a clock like gamma oscillation, which has been observed in the hippocampus \cite{jensen2007human}. Therefore, neuronal oscillations allow for temporal segmentation of neuronal spikes. 
%
Here, we assume that the storage and retrieval of the input patterns can happen in gamma time scale.
%
These gamma oscillations synchronizing spikes and creates pause between items in a message to prevent errors in decoding particularly in the CA3. 
%
The indication is thus that retrieval would be very rapid from the CA3 network, indeed, fast enough for it to be biologically plausible.
%
The dual oscillations are hypothesized to form a code for representing multiple items in an ordered way (Lisman and Idiart, 1995; Jensen and Lisman, 1996). 
%
In order to get something in order of the behavioural time scale which is stored in gamma cycle, we postulate that the inputs to the hippocampus are temporaly compressed due to the phase precession mechanism (O'Keefe and Recce, 1993) out of the hippocampus e. g., phase precession hase been observed in the EC (?). 
%
Temporally compressing behavioral sequences that happen on the time scale of seconds down to the time scale of milliseconds (within a gamma cycle). This compression of behavioral time scale of seconds to the time scale of a theta or gamma cycle enables synaptic plasticity and a neural mechanism for representing temporal order of events required for episodic memory. So a rat's hippocampus compresses ongoing experience into repeating theta sequences. Figure 3. [3] [7] [16] [2] [9] [10] [11] [13][15] [14]
%
This assumptions makes it possible to formulate a model of sequence memory with a time clock running to keep the time steps apart, which is what effectively our simulations implement. 
%
This model is rather abstract and is not a biophysical model of the hippocampus. In principle we aimed to investigate how correlation introduce difficulties to the storage of pattern sequences in the hippocampus.
However, it is out of the scope of this study to suggest which mechanisms is responsible for the storage and retrival of the input memories.   
%



\subsection{Conclusion} 
Compared to previous models, CRISP uses a radically different mechanism for storing episodic memories in the hippocampus. Neural sequences are intrinsic to CA3, and inputs are mapped onto these intrinsic sequences through synaptic plasticity in the feedforward projections of the hippocampus. Here, we computationally investigated, based on the CRISP theory, the role of the complete hippocampal loop in storing and retrieving episodic memories. Our work illustrates how essential it is to consider the whole hippocampal loop while investigating individual functional roles of the subregions. Since a model generating intrinsic sequences in CA3 performs best overall, we conclude that CRISP is a viable theory for the role of the hippocampus in episodic memory.


\bibliographystyle{frontiersinSCNS_ENG_HUMS} % for Science, Engineering and Humanities and Social Sciences articles, for Humanities and Social Sciences articles please include page numbers in the in-text citations
%\bibliographystyle{frontiersinHLTH&FPHY} % for Health, Physics and Mathematics articles
\bibliography{test}

\end{document}

2: Gevins A, Smith ME, McEvoy L, Yu D: High-resolution EEG mapping of
cortical activation related to working memory: effects of task difficulty,
type of processing and practice. Cereb Cortex 1997, 7:374-385.
3. Raghavachari S, Kahana MJ, Rizzuto DS, Caplan JB, Kirschen MP,
Bourgeois B, Madsen R, Lisman JE: Gating of human theta oscillations by
a working memory task. J Neurosci 2001, 21:3175-3183.

5. Holscher C, Anwyl R, Rowan MJ: Stimulation on the positive phase of
hippocampal theta rhythm induces long-term potentiation that can be
depotentiated by stimulation on the negative phase in area CA1 in vivo.
J Neurosci 1997, 17:6470-6477.

6: Jensen O, Kaiser J, Lachaux JP: Human gamma-frequency oscillations
associated with attention and memory. Trends Neurosci 2007, 30:317-324.




\begin{figure}[!htb]
\caption{\textbf{Robust and non-robust generation of sequential activity in models of CA3.} \textbf{A}:~Each column shows the recall performance in a different recurrent network model as labeled on top. Horizontal axes depict the position of the pattern in the sequence (time) and vertical axes show the correlation between each retrieved pattern and the stored one. Each row shows the network performance for recall cues with a particular level of noise $C(y_{1}, y'_{1})$, shown on the righthand side. In each subplot, each line corresponds to a particular sequence. The RCN is highly sensitive to noise, whereas the LCN seems to keep the same input information while retrieving the stored sequence. The PTR shows the best performance and retrieves the previously stored sequence from the noisy cue. The PTG, depending on the amount of correlation between the stored patterns, either completes the sequence to the correct one or confuses and fails the sequence completion. \textbf{B}:~Pattern completion plot. To include the whole range of noise-levels, the pattern completion plot for different network models is shown (see Methods and Materials). Each color in each panel corresponds to one noise-level. Data-points above the diagonal stand for good sequence completion, whereas data below the diagonal show that information in the input is lost. Data on the diagonal show that the network maintains the information in the input cue along the sequence. Overall, the PTR performs best, even with highly corrupted cues. The PCI value for the RCN, LCN, PTR and PTG models are $-0.5, 0.1, 0.56$ and $0.09$, respectively.} 
\label{Fig_3}
\end{figure}

\begin{figure}[!htb]
\caption{\textbf{Visualizing the effects of CA3 dynamics during retrieval.} Each subplot shows the correlation matrix between every retrieved ($\tilde{y}_{l,m}$) and every stored ($y_{l,m}$) pattern, where $l, m = 1, ..., 16$. The networks were initialised with a noisy recall cue with cue quality $C(y_{l,1}, y'_{l,1}) = 0.5$. The gray values show the correlation between the stored and retrieved patterns where white corresponds to a correlation of one and black to zero, respectively. To facilitate the comparison, we overlay a grid of red lines to delineate the boundaries between the sequences. For RCN, all correlation values, except for initial ones on the main diagonal, are close to zero. This means that the network is not robust to input noise but can generate a large number of uncorrelated patterns. For LCN, the high correlation values on the main diagonal show that the network is moderately robust, but on the other hand, the retrieved sequences have a high level of overlap with other sequences (high values in off-diagonal cells). The PTR combines the useful properties of the previous two models, robustness and uncorrelated stored patterns. Only the main diagonal values are close to one and all the other correlations are close to zero, meaning that this network is highly robust to input noise. The PTG, in which patterns are spatially correlated, depending on the level of overlap within and between the stored sequences deteriorates the recall performance.}
\label{Fig_4}
\end{figure}

\begin{figure}[!htb]
\caption{\textbf{Example of recall performance for random input patterns.} Example for sequential retrieval performance of the complete model EC-CA3-CA1-EC with random patterns in EC. Each column shows the results for a different CA3 model, when recall cue with cue quality $C_{EC}(u_1, u'_1) = 0.5$ is provided to EC. Retrieval quality for CA3 (top row), CA1 (middle row), and EC (button row) are shown separately. The horizontal axes represents the position of the pattern in the sequence or time-step. Each colored line shows the correlation between the retrieved and the corresponding stored patterns in a sequence.} 
\label{Fig_5}
\end{figure}

\begin{figure}[!htb]
\caption{\textbf{Summary of pattern and sequence completion for random patterns.} Retrieval quality in different projections for the entire range of noise levels. Each column shows the result for a different CA3 model. Subplots illustrate the pattern completion in the EC-CA3, CA3-CA3, CA3-CA1 and CA1-EC projections. In the last row, the pattern completion is shown by comparing the first and last retrieval correlations of the sequences in EC. Different colors in each panel correspond to different noise-levels.}
\label{Fig_6}
\end{figure}

\begin{figure}[!htb]
\caption{\textbf{Example of recall performance for grid patterns.} Same as Fig. \ref{Fig_5}, but storing grid patterns in the network.}
\label{Fig_7}
\end{figure}

\begin{figure}[!htb]
\caption{\textbf{Summary of pattern and sequence completion for grid patterns.} Same as Fig. \ref{Fig_6}, but storing grid patterns in the network.}
\label{Fig_8}
\end{figure}

\begin{figure}[!htb]
\caption{\textbf{Comparison of retrieval performance at different processing stages.} The simulation is performed with the perfect retrieval cue for random (left) and grid (right) input. Each data point, except for the retrieval cue, shows the average performance in the indicated step.}
\label{Fig_9}
\end{figure}



\begin{figure}[!htb]
\caption{\textbf{Dimensionality of the pattern manifold in different layers.} PCA of the patterns stored in EC, CA1 and CA3 for different network models. The plots show the number of components that are required to account for a total of $85 \%$ of the variance, when stored patterns are random (left column) or derived from grid cells (right column). The top row shows the fraction of the variance explained by individual components, the bottom row shows the cumulative fraction. The manifold of patterns in EC with grid input has much lower dimensionality than random patterns. Since the grid patterns are generated from moving in a 2-d space, the true dimensionality is 2, but the manifold is highly non-linear and PCA cannot extract this information. A similar effect can be seen in CA1 and CA3 with IDN, since these areas are directly driven by EC. CA3 patterns in a LCN, are low dimensional since network activity is constrained by the attractor dynamics to a 2-d manifold.}
\label{Fig_10}
\end{figure}


\begin{figure}[!htb]
\caption{\textbf{How close are retrieved CA1 patterns to the correct one?} Histograms of correlations between retrieved patterns and corresponding stored patterns (cyan) and between retrieved patterns and all other stored patterns (red). Retrieval is initiated with the perfect recall cues. Right and left columns show the results when random or grid patterns are stored, respectively. The number inside each panel shows the confusion rate. That is how often the correlation between the retrieved and the original pattern is smaller than the correlation between retrieved pattern and at least one of the other stored patterns in the network.} 
\label{Fig_11}
\end{figure}

\begin{figure}[!htb]
%\centering\includegraphics[width=1.\linewidth]{Fig_1.eps}
\caption{\textbf{Capacity analysis for random and grid patterns.} 
\textbf{A}:~Recall performance with random input as quantified by the pattern completion index (PCI) in different projections, for the four different CA3 networks as a function of the number of stored sequences. The grey shading of the marker indicates the range of the data points between zero and one that is used to calculate the PCI (e.g., see Fig.~\ref{Fig_8}). White indicates a complete range, black a single data point near zero.
The range is important since a small range implies that the PCI is unreliable.
We define the network capacity in our model as the maximum number of sequences than can be stored in the network such that PCI $> 0$ in the comparison of the last retrieved pattern to the recall cue (last row). \textbf{B}:~Network capacity when grid input is presented to EC.}.   
\label{Fig_12}
\end{figure}


\begin{figure}[!htb]
\centering\includegraphics[width=1.\linewidth]{Fig_1.eps}
\caption{\textbf{Effect of noise in neural dynamics on sequence completion.}
Same plotting convention as in Fig.~\ref{Fig_12}.
 }
\label{Fig_13}
\end{figure}





